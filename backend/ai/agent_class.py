# agent_class.py
import os
import json
import random
import google.generativeai as genai
from dotenv import load_dotenv
from livekit.agents import Agent
from livekit.agents.llm import function_tool
from livekit.rtc import Room, DataPacket
from analyst_agent import generate_analysis_report

load_dotenv()

# --- 1. CONFIGURAÇÃO DA IA E PROMPTS ---
try:
    api_key = os.environ.get("GOOGLE_API_KEY") or os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise KeyError
    genai.configure(api_key=api_key)
except KeyError:
    print("ERRO: GOOGLE_API_KEY ou GEMINI_API_KEY não encontrada no .env")
    exit()

MODEL = genai.GenerativeModel('gemini-1.5-flash')

SOLUTION_FEEDBACK_PROMPT = """
You are a Senior Software Engineer conducting a code review. Your task is to provide constructive feedback on a candidate's solution, using an existing analysis report as a baseline.
**INTERVIEWER'S ANALYSIS REPORT (Your baseline for the optimal solution):**
---
{analysis_report}
---
**CANDIDATE'S SOLUTION (Transcribed from their explanation):**
{user_solution}
**YOUR TASK:**
Analyze the candidate's solution by comparing it with the optimal approach in the report. Generate a final report in Markdown with the following sections: 1. General Feedback, 2. Correctness and Logic, 3. Efficiency (Complexity), 4. Code Quality, 5. Areas for Improvement.
Be professional, encouraging, and technical.
"""

SYSTEM_PROMPT_INTERVIEWER = """
You are an expert interviewer named Ada. You are professional and friendly. Your entire process is a strict, repeating cycle. You MUST follow these steps precisely.
**Interview Flow:**
1. **Greeting:** Start the conversation *only once* with a friendly greeting, introduce yourself, and ask ONE behavioral question.
2. **First Technical Question:** After the candidate answers the behavioral question, you MUST immediately call your `prepare_technical_question` tool.
3. **Conduct Technical Interview:** Use the report from the tool to interview the candidate about this specific problem.
4. **Listen and Conclude Question:** Listen to the candidate's full solution and explanation. When they are finished, you MUST call the `evaluate_solution` tool, passing their transcribed solution to it. Then, deliver the report generated by the tool as your final feedback for that question.
5. **Cycle:** After giving feedback, you can move to the next technical question by calling `prepare_technical_question` again.
Do not end the interview. Continue this cycle until the user decides to end the conversation.
"""

class InterviewAgent(Agent):
    def __init__(self, room: Room):
        super().__init__(instructions=SYSTEM_PROMPT_INTERVIEWER)
        self.room = room
        self.current_question = None
        self.current_report = None

        self.on("data_received", self._on_data_received)

    @function_tool 
    async def prepare_technical_question(self) -> str:

        """
        Gets a new technical problem and calls an external analyst to create a report.
        """
        print("INFO: [Agent] Tool 'prepare_technical_question' called.")
        try:
            with open("data/questions.json", "r", encoding="utf-8") as f:
                questions = json.load(f)
            selected_question = random.choice(questions)
            self.current_question = selected_question

            report = await generate_analysis_report(selected_question)
            self.current_report = report

            frontend_payload = {"type": "SHOW_TECHNICAL_QUESTION", "payload": selected_question}
            message_str = json.dumps(frontend_payload)
            await self.room.local_participant.publish_data(message_str, topic="interview_events")

            return report
        except Exception as e:
            print(f"ERROR: [Agent] in prepare_technical_question: {e}")
            return "Error: Could not prepare the technical question."

    @function_tool
    async def evaluate_solution(self, user_solution: str) -> str:
        """
        Evaluates the candidate's solution using its own internal AI logic.
        """
        print("INFO: [Agent] Tool 'evaluate_solution' called.")
        if not self.current_report:
            return "Error: A technical question's analysis report must be prepared first."
        try:
            prompt = SOLUTION_FEEDBACK_PROMPT.format(
                analysis_report=self.current_report,
                user_solution=user_solution
            )
            response = await MODEL.generate_content_async(prompt)
            return response.text
        except Exception as e:
            print(f"ERROR: [Agent] in evaluate_solution: {e}")
            return "Error: Could not generate the feedback report."
            
    # A lógica interna deste método já estava correta.
    async def _on_data_received(self, dp: DataPacket):
        if dp.topic != "agent_control":
            return
            
        try:
            message = json.loads(dp.data)
            
            if message.get("type") == "SPEAK_EVALUATION_RESULT":
                code = message["payload"]["text"]
                
                feedback_text = await self.evaluate_solution(code)

                if feedback_text:
                    print(f"INFO: [Agent] Sending and speaking feedback.")

                    # AÇÃO 1: ENVIAR O TEXTO PARA O FRONTEND
                    frontend_payload = {
                        "type": "SHOW_EVALUATION_FEEDBACK",
                        "payload": {"text": feedback_text}
                    }
                    await self.room.local_participant.publish_data(
                        json.dumps(frontend_payload),
                        topic="interview_events" 
                    )

                    # AÇÃO 2: FAZER O AGENTE FALAR O TEXTO
                    await self.say(feedback_text)

        except Exception as e:
            print(f"ERROR: [Agent] in _on_data_received: {e}")