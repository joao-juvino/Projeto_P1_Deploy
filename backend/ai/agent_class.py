# agent_class.py
import os
import json
import random
import logging
import google.generativeai as genai
from dotenv import load_dotenv
from livekit.agents import Agent
from livekit.agents.llm import function_tool
from livekit.rtc import Room, DataPacket
from analyst_agent import generate_analysis_report

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# --- CONFIGURAÇÃO DA IA ---
try:
    api_key = os.environ.get("GOOGLE_API_KEY") or os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise KeyError
    genai.configure(api_key=api_key)
except KeyError:
    logger.error("ERRO: GOOGLE_API_KEY ou GEMINI_API_KEY não encontrada no .env")
    exit()

MODEL_LIGHT = genai.GenerativeModel('gemini-1.5-flash')
MODEL_PRO = genai.GenerativeModel('gemini-1.5-pro')

# --- PROMPTS (CORRIGIDOS) ---
PROMPT_MEDIUM = """
You are a Senior Software Engineer reviewing a partial solution.
Compare the candidate's code against the optimal approach, but keep it concise.
Analysis report for optimal solution: {analysis_report}
Candidate's partial solution: {user_solution}
Give feedback with 3 short sections: 1. General Direction, 2. Potential Issues, 3. Suggestion to continue.
Be encouraging and helpful.
"""

PROMPT_FINAL = """
You are an expert coding interviewer creating a final, detailed evaluation.
Evaluate the candidate's final solution based on the provided analysis report of the optimal solution.
Analysis report: {analysis_report}
Candidate's final solution: {user_solution}
Generate a Markdown report covering: 1. Correctness, 2. Complexity (Time & Space), 3. Code Quality, 4. Strengths & Weaknesses, and 5. Final Verdict. Be detailed, professional, and constructive.
"""

SYSTEM_PROMPT_INTERVIEWER = """
You are an expert interviewer named Ada. You are professional and friendly. Your entire process is a strict, repeating cycle. You MUST follow these steps precisely.
**Interview Flow:**
1. **Greeting:** Start the conversation *only once* with a friendly greeting, introduce yourself, and ask ONE behavioral question.
2. **First Technical Question:** After the candidate answers the behavioral question, you MUST immediately call your `prepare_technical_question` tool.
3. **Conduct Technical Interview:** Use the report from the tool to interview the candidate about this specific problem.
4. **Listen and Conclude Question:** Listen to the candidate's full solution and explanation. When they are finished, you MUST call the `evaluate_solution` tool, passing their transcribed solution to it. Then, deliver the report generated by the tool as your final feedback for that question.
5. **Cycle:** After giving feedback, you can move to the next technical question by calling `prepare_technical_question` again.
Do not end the interview. Continue this cycle until the user decides to end the conversation.
"""

class InterviewAgent(Agent):
    def __init__(self, room: Room):
        super().__init__(
            instructions=SYSTEM_PROMPT_INTERVIEWER
        )
        self.room = room
        self.current_question = None
        self.current_report = None
        logger.info("InterviewAgent inicializado e ouvindo eventos.")

    @function_tool 
    async def prepare_technical_question(self) -> str:
        logger.info("Tool 'prepare_technical_question' called.")
        try:
            with open("data/questions.json", "r", encoding="utf-8") as f:
                questions = json.load(f)
            selected_question = random.choice(questions)
            self.current_question = selected_question

            if self.room and self.room.local_participant:
                frontend_payload = {
                    "type": "SHOW_TECHNICAL_QUESTION", 
                    "payload": selected_question
                }
                await self.room.local_participant.publish_data(
                    json.dumps(frontend_payload), 
                    topic="interview_events"
                )

            report = await generate_analysis_report(selected_question)
            self.current_report = report
            
            return report
            
        except Exception as e:
            logger.error(f"Error in prepare_technical_question: {e}")
            return "Error: Could not prepare the technical question."


    async def evaluate_partial_solution(self, user_solution: str) -> str:
        """Avaliação parcial - não é @function_tool"""
        if not self.current_report:
            return "Error: No technical question prepared."
        try:
            prompt = PROMPT_MEDIUM.format(
                analysis_report=self.current_report,
                user_solution=user_solution
            )
            response = await MODEL_LIGHT.generate_content_async(prompt)
            return response.text
        except Exception as e:
            logger.error(f"Error in evaluate_partial_solution: {e}")
            return "Error: Could not generate partial feedback."

    async def evaluate_final_solution(self, user_solution: str) -> str:
        """Avaliação final - não é @function_tool"""
        if not self.current_report:
            return "Error: No technical question prepared."
        try:
            prompt = PROMPT_FINAL.format(
                analysis_report=self.current_report,
                user_solution=user_solution
            )
            response = await MODEL_PRO.generate_content_async(prompt)
            return response.text
        except Exception as e:
            logger.error(f"Error in evaluate_final_solution: {e}")
            return "Error: Could not generate final feedback."
            
    async def on_data_received(self, dp: DataPacket):
        if dp.topic != "agent_control":
            return
            
        try:
            message = json.loads(dp.data)
            msg_type = message.get("type")
            
            if msg_type == "REQUEST_PARTIAL_FEEDBACK":
                code = message.get("payload", {}).get("text", "")
                logger.info("Recebido pedido de feedback parcial.")
                
                await self.say("Analisando seu progresso, um momento...", allow_interruptions=True)
                feedback_text = await self.evaluate_partial_solution(code)
                
                if self.room and self.room.local_participant:
                    frontend_payload = {
                        "type": "PARTIAL_FEEDBACK_RESULT",
                        "payload": {"text": feedback_text}
                    }
                    await self.room.local_participant.publish_data(
                        json.dumps(frontend_payload),
                        topic="interview_events" 
                    )
                    await self.say(feedback_text, allow_interruptions=True)
            
            elif msg_type == "SUBMIT_SOLUTION_FINAL":
                code = message.get("payload", {}).get("text", "")
                logger.info("Recebida submissão de solução final.")
                
                await self.say("Recebi sua solução final. Fazendo análise completa...", allow_interruptions=False)
                feedback_text = await self.evaluate_final_solution(code)
                
                if self.room and self.room.local_participant:
                    frontend_payload = {
                        "type": "FINAL_FEEDBACK_RESULT",
                        "payload": {"text": feedback_text}
                    }
                    await self.room.local_participant.publish_data(
                        json.dumps(frontend_payload),
                        topic="interview_events" 
                    )
                    await self.say(feedback_text, allow_interruptions=False)
            
            elif msg_type == "SPEAK_EVALUATION_RESULT": 
                code = message.get("payload", {}).get("text", "")
                logger.info("Recebido pedido de avaliação (legado).")
                feedback_text = await self.evaluate_final_solution(code)
                
                if self.room and self.room.local_participant:
                    frontend_payload = {
                        "type": "SHOW_EVALUATION_FEEDBACK",
                        "payload": {"text": feedback_text}
                    }
                    await self.room.local_participant.publish_data(
                        json.dumps(frontend_payload),
                        topic="interview_events" 
                    )
                    await self.say(feedback_text)

        except Exception as e:
            logger.error(f"Error in _on_data_received: {e}")